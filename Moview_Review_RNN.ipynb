{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# fname = \"../datasets/tar/aclImdb_v1.tar.gz\"\n",
    "# tar = tarfile.open(fname, \"r:gz\")\n",
    "# tar.extractall()\n",
    "# tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "def prepareData(dir):\n",
    "    data = text_dataset_from_directory(dir)\n",
    "    return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    "  )\n",
    "\n",
    "train_data = prepareData(\"../datasets/Movies/train\")\n",
    "test_data = prepareData(\"../datasets/Movies/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"If you where run over by the Miramax foreign film juggernaut, then you missed this brilliant gem tucked away in one those twenty seats cinema theater.  A film is very much like a painting, meant to be seen not discussed or explained. So let us just leave it at 'see it'.  Benito Zambrano's talent on the other hand merits more than a discussion. A sensitive director and a poignant writer. In many ways 'Solas' reminded me of another gem in the dust 'Heavy'.  Benito managed to keep the movie so simple, that it hurts. His flare for observing and then relaying in his film the raw human angst, is inspiring.  The actors for there part, rose to the greatness of the moment.  BZ makes us cling to hope by our finger nails while steadily adding to our feet the weight of reality. But then, isn't that life!  To look for hope in 'Solas' is to look for simplicity in 'Guernica'. It's there, you just need to see it.  And like all good things in life this one is elusive too. No video or a DVD release yet.  Once again, it lives up to it's name.\"\n",
      "\n",
      "2\n",
      "b\"This show was so short lived that I didn't even get to see every episode. It premiered during the summer of 2002. I saw a couple of episodes. They were funny. It was so coll how they poked fun of the sitcoms of the 70s like The Facts Of Life, The Partridge Family, Different Strokes, and What's Happenin!! There was an episode when they poked fun of the Facts of Life and had a man playing the role of Mrs. Garrett. But I went out of town for less than a week and came back home and the show went off the air for good. No warning. I didn't even get a chance to see every episode! The rerun show never lasted long enough to have reruns!\"\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_data.take(2):\n",
    "    print(text_batch.numpy()[0])\n",
    "    print()\n",
    "    print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# ----- 1. INPUT\n",
    "# We need this to use the TextVectorization layer next.\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 2. TEXT VECTORIZATION\n",
    "\n",
    "# This layer processes the input string and turns it into a sequence of max_len integers, each of which maps to a certain token.\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  # Max vocab size. Any words outside of the max_tokens most common ones\n",
    "  # will be treated the same way: as \"out of vocabulary\" (OOV) tokens.\n",
    "  max_tokens=max_tokens,\n",
    "  # Output integer indices, one per string token\n",
    "  output_mode=\"int\",\n",
    "  # Always pad or truncate to exactly this many tokens\n",
    "  output_sequence_length=max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call adapt(), which fits the TextVectorization layer to our text dataset.\n",
    "# This is when the max_tokens most common words (i.e. the vocabulary) are selected.\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "model.add(vectorize_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3. EMBEDDING\n",
    "\n",
    "# Note that we're using max_tokens + 1 here, since there's an out-of-vocabulary (OOV) token that gets added to the vocab.\n",
    "model.add(Embedding(max_tokens + 1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 4. RECURRENT LAYER\n",
    "\n",
    "model.add(LSTM(64))\n",
    "# 64 is the \"units\" parameter, which is the dimensionality of the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 5. DENSE HIDDEN LAYER\n",
    "model.add(Dense(64, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2344/2344 [==============================] - 99s 42ms/step - loss: -6.3155 - accuracy: 1.3333e-05\n",
      "Epoch 2/10\n",
      "2344/2344 [==============================] - 80s 34ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2344/2344 [==============================] - 74s 32ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2344/2344 [==============================] - 74s 32ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2344/2344 [==============================] - 74s 32ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2344/2344 [==============================] - 75s 32ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2344/2344 [==============================] - 75s 32ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2344/2344 [==============================] - 82s 35ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2344/2344 [==============================] - 100s 43ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2344/2344 [==============================] - 100s 43ms/step - loss: -6.6633 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x272694e78e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- 6. Compile and train the model.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x27165b25040>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_weights('rnn')\n",
    "\n",
    "model.load_weights('rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 28s 36ms/step - loss: 7.6274 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.627357482910156, 0.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- 7. EVALUATE\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.2074769 1.626152  1.2981178 1.4491545 1.7925177 1.5445901 1.701822\n",
      "  2.4309747 2.0220075 1.6786261 1.428061  2.0254533 1.6126689 2.8055391\n",
      "  2.7372334 2.6900656 1.4870291 2.1604598 3.3382287 1.9424106 2.2953417\n",
      "  1.4545922 2.01653   1.6751395 1.5149219 1.4742379 3.03322   1.9492502\n",
      "  1.6977496 0.        1.8107419 1.4547609 1.6398175 1.4038697 1.9365258\n",
      "  1.7763956 3.1605396 1.7768819 2.1136408 2.8857856 1.9248207 1.2178427\n",
      "  1.3763291 2.2874427 1.7787526 1.7716644 1.7903781 2.2046525 2.142461\n",
      "  2.2708464 1.4177294 1.423459  2.067016  3.3653083 1.947506  1.8641466\n",
      "  2.2855113 0.        1.7711399 1.5712162 1.5699801 1.7094971 1.8179137\n",
      "  1.9028884]]\n"
     ]
    }
   ],
   "source": [
    "# ----- 8. PREDICT\n",
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.2074769 1.626152  1.2981178 1.4491545 1.7925177 1.5445901 1.701822\n",
      "  2.4309747 2.0220075 1.6786261 1.428061  2.0254533 1.6126689 2.8055391\n",
      "  2.7372334 2.6900656 1.4870291 2.1604598 3.3382287 1.9424106 2.2953417\n",
      "  1.4545922 2.01653   1.6751395 1.5149219 1.4742379 3.03322   1.9492502\n",
      "  1.6977496 0.        1.8107419 1.4547609 1.6398175 1.4038697 1.9365258\n",
      "  1.7763956 3.1605396 1.7768819 2.1136408 2.8857856 1.9248207 1.2178427\n",
      "  1.3763291 2.2874427 1.7787526 1.7716644 1.7903781 2.2046525 2.142461\n",
      "  2.2708464 1.4177294 1.423459  2.067016  3.3653083 1.947506  1.8641466\n",
      "  2.2855113 0.        1.7711399 1.5712162 1.5699801 1.7094971 1.8179137\n",
      "  1.9028884]]\n",
      "[[3.207477  1.6261523 1.2981175 1.4491541 1.7925181 1.5445901 1.7018219\n",
      "  2.4309747 2.0220075 1.6786261 1.4280611 2.0254533 1.6126689 2.8055391\n",
      "  2.7372336 2.6900656 1.4870292 2.1604595 3.3382287 1.9424106 2.2953422\n",
      "  1.454592  2.0165298 1.6751397 1.514922  1.4742379 3.03322   1.94925\n",
      "  1.6977495 0.        1.8107418 1.4547609 1.6398177 1.4038696 1.9365257\n",
      "  1.7763958 3.1605396 1.7768822 2.1136405 2.8857853 1.9248207 1.2178426\n",
      "  1.3763294 2.2874432 1.7787526 1.7716644 1.7903779 2.2046525 2.1424613\n",
      "  2.2708461 1.4177293 1.4234593 2.067016  3.3653083 1.9475061 1.8641466\n",
      "  2.2855113 0.        1.7711399 1.5712163 1.5699801 1.7094971 1.8179134\n",
      "  1.9028887]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))\n",
    "\n",
    "print(model.predict([\n",
    "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
